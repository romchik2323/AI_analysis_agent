

import json
import os
from typing import Optional, Callable
from dataclasses import asdict

from churn_analyzer import ChurnReport
from rag_engine import ChurnRAGEngine



class LLMProvider:
    def chat(self, messages: list[dict], system: str = "") -> str:
        raise NotImplementedError


class ClaudeProvider(LLMProvider):
    
    def __init__(self, api_key: Optional[str] = None, model: str = "claude-haiku-4-5-20251001"):
        try:
            import anthropic
            self.client = anthropic.Anthropic(
                api_key=api_key or os.getenv("ANTHROPIC_API_KEY")
            )
            self.model = model
            self._available = True
            print(f"‚úÖ Claude provider: {model}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Claude –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self._available = False

    def chat(self, messages: list[dict], system: str = "") -> str:
        if not self._available:
            raise RuntimeError("Claude API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        response = self.client.messages.create(
            model=self.model,
            max_tokens=2048,
            system=system,
            messages=messages
        )
        return response.content[0].text


class OllamaProvider(LLMProvider):
    
    def __init__(self, model: str = "qwen2.5", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        self._available = self._check()

    def _check(self) -> bool:
        try:
            import requests
            r = requests.get(f"{self.base_url}/api/tags", timeout=3)
            models = [m["name"] for m in r.json().get("models", [])]
            available = any(self.model in m for m in models)
            if available:
                print(f"‚úÖ Ollama provider: {self.model}")
            else:
                print(f"‚ö†Ô∏è  Ollama: –º–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. "
                      f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama pull {self.model}")
            return available
        except Exception:
            print("‚ö†Ô∏è  Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
            return False

    def chat(self, messages: list[dict], system: str = "") -> str:
        import requests
        all_messages = []
        if system:
            all_messages.append({"role": "system", "content": system})
        all_messages.extend(messages)

        response = requests.post(
            f"{self.base_url}/api/chat",
            json={"model": self.model, "messages": all_messages, "stream": False},
            timeout=120
        )
        response.raise_for_status()
        return response.json()["message"]["content"]



SYSTEM_PROMPT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ (Customer Churn Analysis).
–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –±–∏–∑–Ω–µ—Å—É –ø–æ–Ω–∏–º–∞—Ç—å, –ø–æ—á–µ–º—É —É—Ö–æ–¥—è—Ç –∫–ª–∏–µ–Ω—Ç—ã, –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—à—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–¥–µ—Ä–∂–∞–Ω–∏—è.

–¢—ã –º–æ–∂–µ—à—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª—é–±–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–µ–π: —Ç–µ–ª–µ–∫–æ–º, –±–∞–Ω–∫–∏, SaaS, e-commerce, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ.
–¢—ã –¥–∞—ë—à—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, actionable —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö.
–¢—ã –æ–±—ä—è—Å–Ω—è–µ—à—å —Å–ª–æ–∂–Ω—ã–µ ML-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º.

–ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: 
- –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º. –ò—Å–ø–æ–ª—å–∑—É–π —á–∏—Å–ª–∞ –∏ —Ñ–∞–∫—Ç—ã."""


class ChurnAgent:
    
    def __init__(
        self,
        provider: str = "auto",
        ollama_model: str = "qwen2.5",
        claude_model: str = "claude-haiku-4-5-20251001",
        use_rag: bool = True
    ):
        self.llm = self._init_provider(provider, ollama_model, claude_model)
        self.rag = ChurnRAGEngine() if use_rag else None
        if self.rag:
            self.rag.initialize()
        self.conversation_history: list[dict] = []
        self.current_report: Optional[ChurnReport] = None

    def _init_provider(self, provider: str, ollama_model: str, claude_model: str) -> LLMProvider:
        if provider == "claude":
            return ClaudeProvider(model=claude_model)
        elif provider == "ollama":
            return OllamaProvider(model=ollama_model)
        elif provider == "auto":
            
            claude = ClaudeProvider(model=claude_model)
            if claude._available:
                return claude
            ollama = OllamaProvider(model=ollama_model)
            if ollama._available:
                return ollama
            print("‚ö†Ô∏è  LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
            return None
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider}")

    
    def analyze_report(self, report: ChurnReport) -> str:
       
        self.current_report = report

        
        rag_context = ""
        if self.rag:
            query = (
                f"–æ—Ç—Ç–æ–∫ {report.churn_rate}% "
                f"—Ñ–∞–∫—Ç–æ—Ä—ã: {', '.join(f['feature'] for f in report.top_factors[:3])}"
            )
            rag_context = self.rag.get_context(query, top_k=3)

        
        report_text = self._format_report_for_llm(report)

        user_message = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤:

{report_text}

{"--- –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---" if rag_context else ""}
{rag_context}

–î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑:
1. –ß—Ç–æ –≥–æ–≤–æ—Ä—è—Ç —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤?
2. –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã?
3. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –æ—Ç—Ç–æ–∫–∞
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π (—á—Ç–æ –¥–µ–ª–∞—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å)"""

        response = self._call_llm(user_message)

        
        if self.rag:
            self.rag.add_analysis(
                f"–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: churn_rate={report.churn_rate}%, "
                f"ROC-AUC={report.roc_auc}, "
                f"top_factor={report.top_factors[0]['feature'] if report.top_factors else 'N/A'}. "
                f"–ê–Ω–∞–ª–∏–∑ LLM: {response[:300]}...",
                metadata={"churn_rate": report.churn_rate, "roc_auc": report.roc_auc}
            )

        return response

    def chat(self, user_message: str) -> str:
        
        rag_context = ""
        if self.rag:
            rag_context = self.rag.get_context(user_message, top_k=2)

        
        context_parts = []
        if self.current_report:
            context_parts.append(
                f"–¢–µ–∫—É—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {self.current_report.total_customers} –∫–ª–∏–µ–Ω—Ç–æ–≤, "
                f"–æ—Ç—Ç–æ–∫ {self.current_report.churn_rate}%, "
                f"ROC-AUC={self.current_report.roc_auc}"
            )
        if rag_context:
            context_parts.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{rag_context}")

        full_message = user_message
        if context_parts:
            full_message = "\n\n".join(context_parts) + f"\n\n–í–æ–ø—Ä–æ—Å: {user_message}"

        response = self._call_llm(full_message)
        return response

    def explain_customer(self, customer_data: dict, churn_prob: float) -> str:
       
        rag_context = ""
        if self.rag:
            query = " ".join(f"{k}={v}" for k, v in list(customer_data.items())[:5])
            rag_context = self.rag.get_context(query, top_k=2)

        message = f"""–ö–ª–∏–µ–Ω—Ç –∏–º–µ–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
{json.dumps(customer_data, ensure_ascii=False, indent=2)}

–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞: {churn_prob:.1f}%

{f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {rag_context}" if rag_context else ""}

–û–±—ä—è—Å–Ω–∏:
1. –ü–æ—á–µ–º—É —É —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ —Ç–∞–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞?
2. –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –Ω–∞–∏–±–æ–ª–µ–µ –≤–ª–∏—è—é—Ç?
3. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è —É–¥–µ—Ä–∂–∞–Ω–∏—è –∏–º–µ–Ω–Ω–æ —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""

        return self._call_llm(message)

    def generate_retention_strategy(self, segment_description: str) -> str:
        
        rag_context = ""
        if self.rag:
            rag_context = self.rag.get_context(
                f"—Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–¥–µ—Ä–∂–∞–Ω–∏—è {segment_description}", top_k=3
            )

        message = f"""–†–∞–∑—Ä–∞–±–æ—Ç–∞–π –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É–¥–µ—Ä–∂–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞:

{segment_description}

{f"–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:{chr(10)}{rag_context}" if rag_context else ""}

–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–ª–∂–Ω–∞ –≤–∫–ª—é—á–∞—Ç—å:
1. –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (–ø–µ—Ä–≤—ã–µ 48 —á–∞—Å–æ–≤)
2. –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –º–µ—Ä—ã (1-2 –Ω–µ–¥–µ–ª–∏)
3. –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (1-3 –º–µ—Å—è—Ü–∞)
4. KPI –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
5. –ü—Ä–∏–º–µ—Ä–Ω—ã–π ROI –æ—Ç —É–¥–µ—Ä–∂–∞–Ω–∏—è"""

        return self._call_llm(message)

    
    def _call_llm(self, user_message: str) -> str:
        self.conversation_history.append({"role": "user", "content": user_message})

        if self.llm is None:
            response = "[LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω] –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ ANTHROPIC_API_KEY"
        else:
            try:
                response = self.llm.chat(
                    messages=self.conversation_history[-10:],  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                    system=SYSTEM_PROMPT
                )
            except Exception as e:
                response = f"–û—à–∏–±–∫–∞ LLM: {e}"

        self.conversation_history.append({"role": "assistant", "content": response})
        return response

    def _format_report_for_llm(self, report: ChurnReport) -> str:
        lines = [
            f"üìä –û–¢–ß–Å–¢ –û–ë –û–¢–¢–û–ö–ï –ö–õ–ò–ï–ù–¢–û–í",
            f"–í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {report.total_customers:,}",
            f"–£—à–ª–æ: {report.churned_customers:,} ({report.churn_rate}%)",
            f"",
            f"–ö–ê–ß–ï–°–¢–í–û –ú–û–î–ï–õ–ò:",
            f"  Accuracy: {report.model_accuracy}%",
            f"  ROC-AUC: {report.roc_auc}",
            f"  Precision: {report.precision:.1%}",
            f"  Recall: {report.recall:.1%}",
            f"  F1: {report.f1:.1%}",
            f"",
            f"–¢–û–ü-5 –§–ê–ö–¢–û–†–û–í –û–¢–¢–û–ö–ê:",
        ]
        for i, factor in enumerate(report.top_factors[:5], 1):
            lines.append(f"  {i}. {factor['feature']}: {factor['importance']:.3f}")

        lines += [
            f"",
            f"–ö–õ–ò–ï–ù–¢–´ –í–´–°–û–ö–û–ì–û –†–ò–°–ö–ê:",
            f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {report.high_risk_count:,}",
            f"  –†–µ–∞–ª—å–Ω—ã–π –æ—Ç—Ç–æ–∫ –≤ –≥—Ä—É–ø–ø–µ: {report.high_risk_churn_rate}%",
            f"",
            f"–°–ò–°–¢–ï–ú–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:",
        ]
        for rec in report.recommendations:
            lines.append(f"  ‚Ä¢ {rec}")

        return "\n".join(lines)

    def reset_conversation(self):
        self.conversation_history = []
        self.current_report = None



if __name__ == "__main__":
    import sys
    sys.path.insert(0, ".")

    provider = sys.argv[1] if len(sys.argv) > 1 else "auto"
    print(f"\nü§ñ –ó–∞–ø—É—Å–∫ ChurnAgent (provider={provider})\n")

    agent = ChurnAgent(provider=provider)

    
    print("üí¨ –ß–∞—Ç —Å –∞–≥–µ–Ω—Ç–æ–º. –í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.\n")
    while True:
        query = input("–í—ã: ").strip()
        if query.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            break
        if not query:
            continue
        response = agent.chat(query)
        print(f"\nü§ñ –ê–≥–µ–Ω—Ç: {response}\n")