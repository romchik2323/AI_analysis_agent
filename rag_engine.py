
import json
import numpy as np
import pickle
from pathlib import Path
from typing import Optional
import warnings
warnings.filterwarnings("ignore")




class SimpleVectorStore:
    

    def __init__(self):
        self.vectors: list[np.ndarray] = []
        self.documents: list[dict] = []

    def add(self, vector: np.ndarray, document: dict):
        self.vectors.append(vector / (np.linalg.norm(vector) + 1e-9))
        self.documents.append(document)

    def search(self, query_vector: np.ndarray, top_k: int = 5) -> list[dict]:
        if not self.vectors:
            return []
        q = query_vector / (np.linalg.norm(query_vector) + 1e-9)
        matrix = np.stack(self.vectors)
        scores = matrix @ q
        top_indices = np.argsort(scores)[::-1][:top_k]
        results = []
        for i in top_indices:
            doc = dict(self.documents[i])
            doc["score"] = float(scores[i])
            results.append(doc)
        return results

    def __len__(self):
        return len(self.documents)




class E5Embedder:
    

    def __init__(self, model_name: str = "intfloat/multilingual-e5-large"):
        self.model_name = model_name
        self._model = None
        self._tokenizer = None
        self._available = False
        self._try_load()

    def _try_load(self):
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self._model = AutoModel.from_pretrained(self.model_name)
            self._model.eval()
            self._torch = torch
            self._available = True
            print(f"‚úÖ E5-large –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  E5-large –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ({e}). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è TF-IDF fallback.")
            self._available = False

    def encode(self, texts: list[str]) -> np.ndarray:
        if self._available:
            return self._encode_e5(texts)
        return self._encode_tfidf(texts)

    def _encode_e5(self, texts: list[str]) -> np.ndarray:
        """E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å 'query: ' –∏–ª–∏ 'passage: '."""
        import torch
        prefixed = [f"passage: {t}" for t in texts]
        inputs = self._tokenizer(
            prefixed, padding=True, truncation=True,
            max_length=512, return_tensors="pt"
        )
        with torch.no_grad():
            outputs = self._model(**inputs)
        # Mean pooling
        token_embs = outputs.last_hidden_state
        attention_mask = inputs["attention_mask"]
        mask_expanded = attention_mask.unsqueeze(-1).float()
        embeddings = (token_embs * mask_expanded).sum(1) / mask_expanded.sum(1)
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        return embeddings.numpy()

    def encode_query(self, text: str) -> np.ndarray:
        """–ö–æ–¥–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 'query: '."""
        if self._available:
            import torch
            inputs = self._tokenizer(
                f"query: {text}", return_tensors="pt",
                truncation=True, max_length=512
            )
            with torch.no_grad():
                outputs = self._model(**inputs)
            emb = outputs.last_hidden_state.mean(dim=1)
            emb = torch.nn.functional.normalize(emb, p=2, dim=1)
            return emb.squeeze().numpy()
        return self._encode_tfidf([text])[0]

    def _encode_tfidf(self, texts: list[str]) -> np.ndarray:
        """Fallback: –ø—Ä–æ—Å—Ç–æ–π TF-IDF –≤–µ–∫—Ç–æ—Ä."""
        from sklearn.feature_extraction.text import TfidfVectorizer
        if not hasattr(self, "_tfidf"):
            self._tfidf = TfidfVectorizer(max_features=512)
            self._tfidf.fit(texts)
        try:
            return self._tfidf.transform(texts).toarray().astype(np.float32)
        except Exception:
            self._tfidf.fit(texts)
            return self._tfidf.transform(texts).toarray().astype(np.float32)



class ChurnRAGEngine:
   
    DOMAIN_KNOWLEDGE = [
        {
            "type": "domain",
            "text": "–ö–ª–∏–µ–Ω—Ç—ã —Å –ø–æ–º–µ—Å—è—á–Ω—ã–º–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏ —É—Ö–æ–¥—è—Ç –≤ 3-4 —Ä–∞–∑–∞ —á–∞—â–µ, "
                    "—á–µ–º –∫–ª–∏–µ–Ω—Ç—ã —Å –≥–æ–¥–æ–≤—ã–º–∏ –∏–ª–∏ –¥–≤—É—Ö–ª–µ—Ç–Ω–∏–º–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏. "
                    "–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–¥–µ—Ä–∂–∞–Ω–∏—è: –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –Ω–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã —Å–æ —Å–∫–∏–¥–∫–æ–π.",
            "tags": ["contract", "retention"]
        },
        {
            "type": "domain",
            "text": "–û–ø—Ç–æ–≤–æ–ª–æ–∫–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç (Fiber optic) –∞—Å—Å–æ—Ü–∏–∏—Ä—É–µ—Ç—Å—è —Å –≤—ã—Å–æ–∫–∏–º –æ—Ç—Ç–æ–∫–æ–º ~42%. "
                    "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –≤—ã—Å–æ–∫–∞—è —Ü–µ–Ω–∞, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è. "
                    "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞ –∏ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–∞—Ä–∏—Ñ—ã.",
            "tags": ["internet", "fiber", "pricing"]
        },
        {
            "type": "domain",
            "text": "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –æ—Ç—Ç–æ–∫ —Å 15% –¥–æ 42%. "
                    "–¢–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞ ‚Äî –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö —Å–∏–ª—å–Ω—ã—Ö —É–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤. "
                    "–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –æ—Ç—Ç–æ–∫ –Ω–∞ 61%.",
            "tags": ["techsupport", "retention"]
        },
        {
            "type": "domain",
            "text": "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —á–µ–∫–∏ (electronic check) —Å–≤—è–∑–∞–Ω—ã —Å 45% –æ—Ç—Ç–æ–∫–∞. "
                    "–≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –Ω–∏–∑–∫—É—é –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏. "
                    "–ö–ª–∏–µ–Ω—Ç—ã –Ω–∞ –∞–≤—Ç–æ–ø–ª–∞—Ç—ë–∂–µ (–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞, –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥) —É—Ö–æ–¥—è—Ç —Ä–µ–∂–µ.",
            "tags": ["payment", "electronic_check"]
        },
        {
            "type": "domain",
            "text": "Tenure (–≤—Ä–µ–º—è —Å –∫–æ–º–ø–∞–Ω–∏–µ–π) ‚Äî —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏. "
                    "–ö–ª–∏–µ–Ω—Ç—ã –ø–µ—Ä–≤—ã—Ö 6 –º–µ—Å—è—Ü–µ–≤ ‚Äî –≥—Ä—É–ø–ø–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∏—Å–∫–∞ (onboarding churn). "
                    "–ü—Ä–æ–≥—Ä–∞–º–º—ã onboarding –∏ early engagement –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã.",
            "tags": ["tenure", "onboarding", "loyalty"]
        },
        {
            "type": "domain",
            "text": "SaaS-–∫–æ–º–ø–∞–Ω–∏–∏: –æ—Ç—Ç–æ–∫ >5% –≤ –º–µ—Å—è—Ü –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è —Ä–æ—Å—Ç–∞. "
                    "–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –ø–ª–æ—Ö–æ–π onboarding, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ value realization, "
                    "–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. NPS < 30 –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –≤—ã—Å–æ–∫–∏–º –æ—Ç—Ç–æ–∫–æ–º.",
            "tags": ["saas", "metrics"]
        },
        {
            "type": "domain",
            "text": "–ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ –∫–ª–∏–µ–Ω—Ç—ã: –æ—Ç—Ç–æ–∫ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤. "
                    "–ö–ª–∏–µ–Ω—Ç—ã —Å 1 –ø—Ä–æ–¥—É–∫—Ç–æ–º —É—Ö–æ–¥—è—Ç –≤ 2-3 —Ä–∞–∑–∞ —á–∞—â–µ, —á–µ–º —Å 3+. "
                    "Cross-sell –∏ up-sell ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–¥–µ—Ä–∂–∞–Ω–∏—è –≤ banking.",
            "tags": ["banking", "cross-sell"]
        },
        {
            "type": "domain",
            "text": "E-commerce: –æ—Ç—Ç–æ–∫ –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ RFM-–º–µ—Ç—Ä–∏–∫–∞–º. "
                    "Recency > 90 –¥–Ω–µ–π ‚Äî —Å–∏–≥–Ω–∞–ª —Ç—Ä–µ–≤–æ–≥–∏. "
                    "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç 20-30% dormant-–∫–ª–∏–µ–Ω—Ç–æ–≤.",
            "tags": ["ecommerce", "rfm"]
        },
        {
            "type": "strategy",
            "text": "–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–¥–µ—Ä–∂–∞–Ω–∏—è: —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–π –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç—Ç–æ–∫–∞, "
                    "—Ä–∞—Å—Å—á–∏—Ç–∞–π LTV –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –±—é–¥–∂–µ—Ç –Ω–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ "
                    "–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ LTV √ó –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å_–æ—Ç—Ç–æ–∫–∞.",
            "tags": ["strategy", "ltv", "segmentation"]
        },
        {
            "type": "strategy",
            "text": "–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–µ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ. "
                    "–ö–æ–Ω—Ç–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –∫–ª–∏–µ–Ω—Ç–æ–º –Ω—É–∂–Ω–æ –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –æ–Ω –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ —É–π—Ç–∏. "
                    "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ: 2-4 –Ω–µ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞ —Ä–∏—Å–∫–∞.",
            "tags": ["strategy", "proactive", "timing"]
        },
    ]

    def __init__(self, store_path: str = "rag_store.pkl"):
        self.store_path = Path(store_path)
        self.embedder = E5Embedder()
        self.store = SimpleVectorStore()
        self._initialized = False

    def initialize(self):
        
        if self.store_path.exists():
            self._load_store()
        else:
            self._build_initial_store()
        self._initialized = True
        print(f"üîç RAG Engine –≥–æ—Ç–æ–≤: {len(self.store)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ")

    def _build_initial_store(self):
        texts = [doc["text"] for doc in self.DOMAIN_KNOWLEDGE]
        embeddings = self.embedder.encode(texts)
        for doc, emb in zip(self.DOMAIN_KNOWLEDGE, embeddings):
            self.store.add(emb, doc)
        self._save_store()

    def _save_store(self):
        with open(self.store_path, "wb") as f:
            pickle.dump(self.store, f)

    def _load_store(self):
        with open(self.store_path, "rb") as f:
            self.store = pickle.load(f)

    def add_analysis(self, analysis_text: str, metadata: dict = None):
        
        doc = {
            "type": "analysis",
            "text": analysis_text,
            "metadata": metadata or {}
        }
        emb = self.embedder.encode([analysis_text])[0]
        self.store.add(emb, doc)
        self._save_store()

    def search(self, query: str, top_k: int = 3) -> list[dict]:
        
        if not self._initialized:
            self.initialize()
        query_emb = self.embedder.encode_query(query)
        results = self.store.search(query_emb, top_k=top_k)
        return results

    def get_context(self, query: str, top_k: int = 3) -> str:
        
        docs = self.search(query, top_k=top_k)
        if not docs:
            return ""
        context_parts = []
        for i, doc in enumerate(docs, 1):
            score = doc.get("score", 0)
            context_parts.append(
                f"[–ó–Ω–∞–Ω–∏–µ {i} | —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}]\n{doc['text']}"
            )
        return "\n\n".join(context_parts)